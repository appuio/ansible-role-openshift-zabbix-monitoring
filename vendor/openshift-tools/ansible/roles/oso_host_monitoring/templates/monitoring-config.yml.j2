# settings/vars for host monitoring container startup/configuration
#

---
oso_cluster_id: "{{ osohm_cluster_id }}"
oso_host_type: "{{ osohm_host_type }}"
oso_region: "{{ osohm_region }}"

{% if osohm_host_type == 'master' and osohm_cloud == 'aws' %}
oso_snapshot_aws_key_id: "{{ osohm_snapshot_aws_access_key_id }}"
oso_snapshot_aws_secret_access_key: "{{ osohm_snapshot_aws_secret_access_key }}"
{% elif osohm_host_type == 'master' and osohm_cloud == 'gcp' %}
gcp_volume_user_creds: '{{ osohm_snapshot_gcp_creds | to_json }}'
{% endif %}

{% if osohm_cloud == 'aws' %}
oso_ops_monitoring_aws_key_id: "{{ osohm_ops_monitoring_aws_access_key_id }}"
oso_ops_monitoring_aws_secret_access_key: "{{ osohm_ops_monitoring_aws_secret_access_key }}"
{% endif %}

metric_sender_config:
  host:
    name: "{{ osohm_host_name }}"
  zagg:
    active: True
    url: "{{ osohm_zagg_web_url }}"
    user: "{{ osohm_default_zagg_server_user }}"
    pass: "{{ osohm_default_zagg_server_password }}"
    ssl_verify: "{{ osohm_zagg_verify_ssl }}"
    verbose: False
    debug: False
  hawk:
    active: {{ osohm_hawk_config.active }}
    url: "{{ osohm_hawk_config.url }}"
    user: "{{ osohm_hawk_config.user }}"
    pass: "{{ osohm_hawk_config.pass }}"
    ssl_verify: {{ osohm_hawk_config.ssl_verify }}
    verbose: {{ osohm_hawk_config.verbose }}
    debug: {{ osohm_hawk_config.debug }}
  pcp:
    metrics:
    - hinv.ncpu
    - kernel.all.load
    - kernel.all.pswitch
    - kernel.all.uptime
    - kernel.uname.distro
    - kernel.uname.machine
    - kernel.uname.nodename
    - kernel.uname.nodename
    - kernel.uname.release
    - kernel.uname.sysname
    - kernel.uname.version
    - mem.freemem
    - mem.physmem
    - mem.util.available
    - mem.util.bufmem
    - mem.util.cached
    - mem.util.used
    - proc.nprocs
    - swap.free
    - swap.length
    - swap.used

  heartbeat:
    templates:
    - Template Heartbeat           # So we can send hearbeats
    - Template OS Linux            # So we can send host related metrics
    - Template Config Loop Client  # So we can send config loop client related metrics
{% if osohm_host_type == 'node' and osohm_monitor_dnsmasq|bool %}
    - Template dnsmasq          # So we can monitor dnsmasq
{% endif %}
    - Template Docker           # So we can send docker related metrics
{% if osohm_host_type == 'master' or osohm_host_type == 'node' %}
    - Template Openshift Node   # So we can send node related metrics
{% endif %}
    - Template Operations Tools # So ops-runner works
    - Template Performance Copilot # So we can report PCP metrics
{% if osohm_host_type == 'master' %}
    - Template Openshift Master # So we can send master related metrics
{%   if osohm_enable_cluster_capacity_triggers | bool %}
    - Template Openshift Master Capacity Checks # To notify on low capacity
{%   endif %}
{%   if osohm_cloud == 'gcp' %}
    - Template GCP
{%   elif osohm_cloud == 'aws' %}
    - Template AWS
{%   endif %}
{% if osohm_master_primary | default(False) | bool %}
    - Template OpenShift Metrics
{% endif %}
    - Template OpenShift Logging
{% endif %}
{% if osohm_host_type == 'ansible-tower' %}
    - Template Config Loop      # So we can send config loop related metrics
    - Template Zabbix Config    # So we can send Zabbix Config metrics
    - Template Multi-Inventory  # So we can send multi-inventory failures
{% endif %}

    hostgroups:
    - "{{ osohm_cluster_id }}"
    - "{{ osohm_environment }}"

  synthetic_clusterwide:
    host:
      name: "{{ osohm_cluster_id }}-synthetic"
    heartbeat:
      templates:
      - Template Heartbeat         # So we can send hearbeats
      - Template OpenShift Cluster # So we can hold clusterwide items

# Generic Linux Checks
host_monitoring_cron:
- name: send pcp ping
  minute: "*/10"
  job: ops-runner -f -s 60 -n cspp.pcp.ping /usr/bin/cron-send-pcp-ping

- name: run pcp checks
  minute: "*/5"
  job: ops-runner -f -s 15 -n ozpc.send.pcp /usr/bin/ops-metric-pcp-client

- name: run pcp sampler
  minute: "*/5"
  job: ops-runner -f -s 15 -n cspsm.kernal.all.cpu /usr/bin/cron-send-pcp-sampled-metrics -m kernel.all.cpu.idle -m kernel.all.cpu.nice -m kernel.all.cpu.steal -m kernel.all.cpu.sys -m kernel.all.cpu.user -m kernel.all.cpu.wait.total -m kernel.all.cpu.irq.hard -m kernel.all.cpu.irq.soft

- name: Do a full heartbeat
  minute: "10"
  hour: "*/4"
  job: ops-runner -f -s 300 -n ozc.send.heartbeat.full /usr/bin/ops-metric-client --send-heartbeat

# This is our heart beater and lets us know when a box is down. Too much time
# between checks means it'll take longer for us to know a box is down. See
# heartbeat trigger for details.
- name: Do a quick heartbeat
  minute: "*/3"
  job: ops-runner -f -s 60 -n ozc.send.heartbeat.quick /usr/bin/ops-metric-client -k heartbeat.ping -o 1

- name: send cgroup slice metrics
  minute: "*/15"
  job: ops-runner -f -s 60 -n cscsm.mem.slice /usr/bin/cron-send-cgroup-slice-metrics

{% if ( osohm_host_type == 'master' ) %}
- name: run filesystem space checks
  minute: "*/30"
  job: ops-runner -f -s 120 -n csfm.filesys.full /usr/bin/cron-send-filesystem-metrics
{% elif (osohm_host_type == 'node') and (osohm_sub_host_type == 'infra') %}
- name: run filesystem space checks
  minute: "*/30"
  job: ops-runner -f -s 120 -n csfm.filesys.full /usr/bin/cron-send-filesystem-metrics
{% else %}
- name: run filesystem space checks
  minute: "*/30"
  job: ops-runner -f -s 120 -n csfm.filesys.full /usr/bin/cron-send-filesystem-metrics --filter-pod-pv
{% endif %}

- name: run disk TPS checks
  minute: "*"
  job: ops-runner -f -s 15 -n csdim.disk.tps /usr/bin/cron-send-disk-metrics

- name: run network checks
  minute: "*/10"
  job: ops-runner -f -s 15 -n csnm.network.int /usr/bin/cron-send-network-metrics

- name: report rpm versions daily
  hour: "1"
  minute: "0"
  job: ops-runner -f -s 120 -n csdov.rpm.versions /usr/bin/cron-send-docker-oc-versions

# We might want to break docker checks out at some point.
- name: run docker storage space checks
  minute: "*/15"
  job: ops-runner -f -s 120 -n csdm.docker.storage /usr/bin/cron-send-docker-metrics

# This check shows us when docker is in a bad state (either locked up or too
# slow to respond).
- name: run docker info timer
  minute: "*/10"
  job: ops-runner -f -s 120 -n csdt.docker.timer /usr/bin/cron-send-docker-timer

# This needs to be run every other minute so that we can see accurate depictions of
# relevant container's usage. Otherwise we might miss CPU or Memory spikes.
- name: send conatiner usage metrics
  minute: "*/2"
  job: ops-runner -f -s 15 -n csdcu.docker.containers.usage /usr/bin/cron-send-docker-containers-usage

- name: check for pending security updates once per day
  minute: "0"
  hour:   "10"
  # 43200 is 12 hours. Spread the checks out across a half day
  job: ops-runner -f -s 43200 -n cssuc.security.updates.count /usr/bin/cron-send-security-updates-count

{% if osohm_host_type != 'ansible-tower' %}
# This is an active issue, we need it to run on a frequent basis.
- name: run docker dns test
  minute: "*/10"
  job: source $HOME/.bashrc; ops-runner -f -s 60 -n csddr.docker.dns.resolution /usr/bin/cron-send-docker-dns-resolution
{% endif %}

# This is an active issue, we need it to run on a frequent basis.
# disabling this check due to security concerns
#- name: run docker dns test on existing containers
#  minute: "*/5"
#  job: ops-runner -f -s 60 -n csdedr.docker.existing.dns.resolution /usr/bin/cron-send-docker-existing-dns-resolution

{% if osohm_host_type == 'node' and osohm_monitor_dnsmasq|bool %}
{# dnsmasq checks #}
- name: send dnsmasq process count
  minute: "*/30"
  job: ops-runner -f -s 15 -n cspc.openshift.dnsmasq.process.count /usr/bin/cron-send-process-count '^dnsmasq' openshift.dnsmasq.process.count

- name: send openshift-node dnsmasq status
  minute: "*/5"
  job: ops-runner -f -s 50 -n csosms.openshift.dnsmasq /usr/bin/cron-send-os-dnsmasq-checks

{# end if dnsmasq #}
{% endif %}

{% if ( osohm_host_type == 'master' or osohm_host_type == 'infra' ) and osohm_cloud == 'aws' %}
- name: send ELB status
  minute: "*/15"
  job: ops-runner -f -s 60 -n cses.openshift.aws.elbd.status /usr/bin/cron-send-elb-status
{% endif %}

{% if osohm_host_type == 'master' or osohm_host_type == 'node' %}
{# Openshift node checks #}
- name: send openshift-node process count
  minute: "*/30"
  job: ops-runner -f -s 15 -n cspc.openshift.node.process.count /usr/bin/cron-send-process-count '^/usr/bin/openshift start node' openshift.node.process.count

- name: send ovs status data
  minute: "*/5"
  job: ops-runner -f -s 60 -n csos.openshift.master.ovs.status /usr/bin/cron-send-ovs-status

- name: fix and report on stray OVS rules
  hour: "*"
  job: ops-runner -f -s 60 -n cfor.openshift.node.ovs.stray.rules /usr/bin/cron-fix-ovs-rules

- name: send cluster docker registry checks
  minute: "*/5"
  job: ops-runner -f -s 15 -n csrc.openshift.node.registry.checks /usr/bin/cron-send-registry-checks

- name: send openshift-node cpu mem per process stats
  minute: "*/5"
  job: ops-runner -f -s 60 -n cscpm.openshift.node.process.cpu.mem.stats /usr/bin/cron-send-cpu-mem-stats 'openshift start node' openshift.node.process

- name: send docker daemon cpu mem per process stats
  minute: "*/5"
  job: ops-runner -f -s 60 -n cscpm.openshift.node.docker.daemon.cpu.mem.stats /usr/bin/cron-send-cpu-mem-stats 'docker-current daemon' openshift.node.docker.daemon

- name: iptables FORWARD chain ordering band-aid script
  minute: "*"
  job: ops-runner -f -s 15 -n iptba.foward.chain.ordering.bandaid /root/forward_chain_bandaid.py
{# end if host master or node #}
{% endif %}

{% if osohm_host_type == 'master' and osohm_saml_auth|bool %}
{# Openshift osohm_saml_auth check #}
- name: check the saml pod status
  minute: "*/5"
  job: ops-runner -f -s 15 -n csss.openshift.master.saml.check /usr/bin/cron-send-saml-status
{# end of Openshift osohm_saml_auth check #}
{% endif %}

{% if osohm_sub_host_type == 'infra' %}
- name: haproxy pruner
  minute: "*/5"
  job: "ops-runner -f -s 60 -n chcw.openshift.node.infra.haproxy /usr/bin/cron-haproxy-close-wait"
{% endif %}

{% if osohm_host_type == 'master' %}
{# Openshift Master checks #}
{% if osohm_enable_app_create|bool %}
- name: run create app
  minute: "*/10"
  job: "ops-runner -f -t 480 -s 60 -n csca.openshift.master.app.create /usr/bin/cron-send-create-app -v --basename pull --source openshift/hello-openshift:v1.0.6 &>> /var/log/create_app.log"

# TODO: This might be fixed, investigate if it's still an issue and possibly remove cron job if it has been fixed.
- name: run Terminating status project
  minute: "*/5"
  job: "ops-runner -f -t 180 -s 60 -n csps.openshift.master.project.terminating.status /usr/bin/cron-send-project-stats"

- name: run create app with build process
  minute: "*/30"
  job: "ops-runner -f -t 1300 -s 120 -n csca.openshift.master.app.build.create /usr/bin/cron-send-create-app -v --basename build --loopcount 180 --source https://github.com/openshift/nodejs-ex &>> /var/log/build_app.log"
{% endif %}

- name: check the dnsmasq status every 5 minutes
  minute: "*/5"
  job: ops-runner -f -t 180 -s 60 -n csdc.openshift.master.dnsmasq.status.check /usr/bin/cron-send-dnsmasq-check

{% if osohm_master_ha|bool %}
- name: send openshift-master process count
  minute: "*/30"
  job: ops-runner -f -s 15 -n cspc.openshift.master.process.count /usr/bin/cron-send-process-count '^/usr/bin/openshift start master controllers' openshift.master.process.count
{% endif %}

{% if not osohm_master_ha|bool %}
- name: send openshift-master process count
  minute: "*/30"
  job: ops-runner -f -s 15 -n cspc.openshift.master.process.count /usr/bin/cron-send-process-count '^/usr/bin/openshift start master' openshift.master.process.count
{% endif %}

- name: send openshift-master local (test https://127.0.0.1) status
  minute: "*/10"
  job: ops-runner -f -s 60 -n csosmm.openshift.master.api.local /usr/bin/cron-send-os-master-metrics --local

- name: send openshift-master SkyDNS status
  minute: "*/5"
  job: ops-runner -f -s 50 -n csosms.openshift.master.skydns /usr/bin/cron-send-os-skydns-checks

- name: send openshift-master etcd status
  minute: "*/5"
  job: ops-runner -f -s 15 -n cspc.openshift.master.etcd.status /usr/bin/cron-send-etcd-status -c /etc/openshift_tools/etcd_metrics.yml

- name: send etcd connections count on port 2379
  minute: "*/10"
  job: ops-runner -f -s 60 -n cscc.openshift.master.etcd.connections.2379 /usr/bin/cron-send-connection-count etcd 2379 openshift.master.etcd.port_2379.connections.established

- name: send etcd connections count on port 2380
  minute: "*/10"
  job: ops-runner -f -s 60 -n cscc.openshift.master.etcd.connections.2380 /usr/bin/cron-send-connection-count etcd 2380 openshift.master.etcd.port_2380.connections.established

- name: send master api server connections count on port 443
  minute: "*/10"
  job: ops-runner -f -s 60 -n cscc.openshift.master.api.connections.443 /usr/bin/cron-send-connection-count openshift 443 openshift.master.api.port_443.connections.established

- name: send openshift master api cpu mem per process stats
  minute: "*/5"
  job: ops-runner -f -s 60 -n cscpm.openshift.master.api.process.cpu.mem.stats /usr/bin/cron-send-cpu-mem-stats 'openshift start master api' openshift.master.api.process

- name: send openshift master controllers cpu mem per process stats
  minute: "*/5"
  job: ops-runner -f -s 60 -n cscpm.openshift.master.controllers.process.cpu.mem.stats /usr/bin/cron-send-cpu-mem-stats 'openshift start master controllers' openshift.master.controllers.process

- name: send etcd cpu mem per process stats
  minute: "*/5"
  job: ops-runner -f -s 60 -n cscpm.openshift.etcd.process.cpu.mem.stats /usr/bin/cron-send-cpu-mem-stats etcd openshift.etcd.process

- name: "OpenShift event watcher"
  hour: "*"
  minute: "*/2"
  job: ops-runner --flock-no-fail -n oscew.event.watcher /usr/bin/cron-event-watcher

{% if osohm_enable_cluster_capacity_reporting|bool %}
- name: "Cluster capacity reporting"
  hour: "*"
  minute: "15"
  job: ops-runner -f -s 120 -n cscc.openshift.master.capacity /usr/bin/cron-send-cluster-capacity
{% endif %}

{% if osohm_master_primary | default(False) | bool %}
- name: send services count
  minute: "*/10"
  job: ops-runner -f -s 30 0n csswc.openshift.master.service.web.check /usr/bin/cron-send-service-web-check --service-count

- name: send openshift-master /healthz status
  minute: "*/5"
  job: ops-runner -f -s 15 -n csosmm.openshift.master.api.healthz /usr/bin/cron-send-os-master-metrics --healthz --api-ping --metrics --node-checks -v &>> /var/log/csosmm-notready.log

- name: Do a full heartbeat for synthetic host
  minute: "10"
  hour: "*/4"
  job: ops-runner -f -s 300 -n ozc.send.synthetic.heartbeat.full /usr/bin/ops-metric-client --send-heartbeat --synthetic

- name: Do a quick heartbeat for synthetic host
  minute: "*/3"
  job: ops-runner -f -s 60 -n ozc.send.synthetic.heartbeat.quick /usr/bin/ops-metric-client -k heartbeat.ping -o 1 --synthetic

{% if osohm_deployment_type == 'dedicated' %}
- name: send  the pv usage status
  minute: "*/5"
  job: ops-runner -f -s 15 -n csup.openshift.master.pv.usage /usr/bin/cron-send-usage-pv
{% endif %}

- name: OpenShift Cluster router stats
  minute: "*/10"
  job: ops-runner -f -s 60 -t 120 -n csors.cluster.router.status /usr/bin/cron-send-os-router-status

- name: Delete stuck projects for bz 1367432
  minute: "*/10"
  job: /usr/bin/delete-stuck-projects &>> /var/log/delete-stuck-projects

- name: Do a quick check for the internal pods status and location
  minute: "15"
  job: ops-runner -f -s 15 -n csipc.openshift.internal.status /usr/bin/cron-send-internal-pods-check

- name: Do a quick check for the project creation and deletion
  minute: "*/15"
  job: ops-runner -f -s 15 -n cspo.openshift.project.status /usr/bin/cron-send-project-operation

- name: send openshift-master counts (user, pod, project)
  hour: "*/2"
  minute: "0"
  job: ops-runner -f -s 15 -n csosmm.openshift.master.counts /usr/bin/cron-send-os-master-metrics --project-count --pod-count --user-count --pv-info


{% if osohm_pruning.pruning_enabled | default(True) | bool %}
- name: "Prune builds/deployments/images"
  hour: "{{ osohm_pruning.cron.hour }}"
  minute: "{{ osohm_pruning.cron.minute }}"
  job: "ops-runner -f -s 30 -n cosp.openshift.master.prune /usr/bin/cron-openshift-pruner --image-keep-younger-than {{ osohm_pruning.image_hours_to_keep }} --image-keep-tag-revisions {{ osohm_pruning.image_revisions_to_keep }} >> /var/log/pruner.log 2>&1"
{% endif %}

- name: "Report on certificate expiration dates"
  hour: "1"
  minute: "1"
  job: ops-runner -f -s 30 -n ccexp.openshift.master.certificates /usr/bin/cron-certificate-expirations --cert-list=/etc/origin/master,/etc/origin/node,/etc/etcd

- name: "Report counts of builds"
  minute: "*/10"
  job: "ops-runner -f -s 30 -n csbc.openshift.build.count cron-send-build-counts"

- name: "Report number of stuck new builds"
  minute: "*/15"
  job: "ops-runner -f -s 30 -n cssb.openshift.stuck_builds.new cron-send-stuck-builds -s New"

- name: "Report number of stuck pending builds"
  minute: "*/15"
  job: "ops-runner -f -s 30 -n cssb.openshift.stuck_builds.pending cron-send-stuck-builds -s Pending"

- name: "Report metrics health"
  hour: "*"
  minute: "*/30"
  job: "ops-runner -f -s 30 -n cosp.openshift.metrics.check cron-send-metrics-checks"

- name: "Report logging health"
  hour: "*"
  minute: "30"
  job: "ops-runner -f -s 30 -n cosp.openshift.logging.check cron-send-logging-checks"

#- name: "Report time to reload haproxy config"
#  hour: "*"
#  minute: "*/10"
#  job: "ops-runner -f -s 30 -n csrrt.openshift.haproxy.check cron-send-router-reload-time"

{% if osohm_monitor_zabbix_infra | default(False) | bool %}
- name: "Check Zabbix repo to see if newer versions are available"
  minute: "1"
  hour: "8"
  job: ops-runner -f -n cszto.zabbix.too.old cron-send-zabbix-too-old
{% endif %}

{% if osohm_cloud == 'aws' %}
- name: send S3 bucket metrics every day
  hour: "3"
  minute: "0"
  job: ops-runner -f -s 120 -n cssm.aws /usr/bin/cron-send-s3-metrics

- name: "Check EBS stuck volume attachment state"
  minute: "*/2"
  job: "ops-runner -f -s 15 -n cseeviss.ebs.stuck.state cron-send-ec2-ebs-volumes-in-stuck-state --region {{ osohm_region }} --stuck-after 120 --aws-creds-profile snapshotter --verbose"

- name: "Add snapshot tag to autoprovisioned pv volumes"
  hour: "*"
  minute: "30"
  job: ops-runner -f -s 120 -n oeasttev.add.snapshot.tag.autopv ops-ec2-add-snapshot-tag-to-ebs-volumes --autoprovisioned-pv-volumes daily --set-purpose-tag --aws-creds-profile snapshotter

- name: "Snapshot volumes tagged with hourly"
  hour: "*"
  minute: "0"
  job: ops-runner -f -s 120 -n oesev.snapshot.hourly ops-ec2-snapshot-ebs-volumes --aws-creds-profile snapshotter --with-schedule hourly --region {{ osohm_region }}

- name: "Snapshot EBS volumes tagged with daily"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n oesev.snapshot.daily ops-ec2-snapshot-ebs-volumes --aws-creds-profile snapshotter --with-schedule daily --region {{ osohm_region }}

- name: "Snapshot EBS volumes tagged with weekly"
  weekday: "0"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n oesev.snapshot.weekly ops-ec2-snapshot-ebs-volumes --aws-creds-profile snapshotter --with-schedule weekly --region {{ osohm_region }}

- name: "Snapshot EBS volumes tagged with monthly"
  day: "1"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n oesev.snapshot.monthly ops-ec2-snapshot-ebs-volumes --aws-creds-profile snapshotter --with-schedule monthly --region {{ osohm_region }}

# DO NOT CHANGE without first talking to twiest.
- name: "Trim EBS Snapshot"
  minute: "10"
  job: ops-runner -f -s 120 -n oetes.trim.snapshots ops-ec2-trim-ebs-snapshots --aws-creds-profile snapshotter --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 1 --delete-orphans-older-than 30 --region {{ osohm_region }}

- name: send AWS Tag checks (config loop)
  hour: "*/6"
  minute: "8"
  job: "ops-runner -f -s 30 -n oect.ec2.tags /usr/bin/ops-ec2-check-tags --aws-creds-profile ops_monitoring --clusterid {{ osohm_cluster_id }}"

{# end if cloud aws #}
{% endif %}
{% if osohm_cloud == 'gcp' %}
- name: send gcs bucket metrics
  hour: "3"
  minute: "0"
  job: ops-runner -f -s 120 -n csgm.gcp /usr/bin/cron-send-gcs-metrics

- name: "Add snapshot tag to autoprovisioned pv volumes"
  hour: "*"
  minute: "30"
  job: ops-runner -f -s 120 -n ogasttpv.add.snapshot.tag.autopv ops-gcp-add-snapshot-label-to-pd-volumes --autoprovisioned-pv-volumes daily --set-purpose-label --gcp-creds-file /root/.gce/creds.json

- name: "Snapshot volumes tagged with hourly"
  hour: "*"
  minute: "0"
  job: ops-runner -f -s 120 -n ogspv.snapshot.hourly ops-gcp-snapshot-pd-volumes --gcp-creds-file /root/.gce/creds.json --with-schedule hourly

- name: "Snapshot pd volumes tagged with daily"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n ogspv.snapshot.daily ops-gcp-snapshot-pd-volumes --gcp-creds-file /root/.gce/creds.json --with-schedule daily

- name: "Snapshot pd volumes tagged with weekly"
  weekday: "0"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n ogspv.snapshot.weekly ops-gcp-snapshot-pd-volumes --gcp-creds-file /root/.gce/creds.json --with-schedule weekly

- name: "Snapshot pd volumes tagged with monthly"
  day: "1"
  hour: "0"
  minute: "0"
  job: ops-runner -f -s 120 -n ogspv.snapshot.monthly ops-gcp-snapshot-pd-volumes --gcp-creds-file /root/.gce/creds.json --with-schedule monthly

- name: "Trim pd Snapshot"
  minute: "10"
  job: ops-runner -f -s 120 -n ogtps.trim.snapshots ops-gcp-trim-pd-snapshots --gcp-creds-file /root/.gce/creds.json --keep-hourly 24 --keep-daily 7 --keep-weekly 4 --keep-monthly 1

- name: send GCP Tag checks (config loop)
  hour: "*/6"
  minute: "8"
  job: "ops-runner -f -s 30 -n ogct.gcp.tags /usr/bin/ops-gcp-check-tags --gcp-creds-file /root/.gce/creds.json --region {{ osohm_region }} --clusterid {{ osohm_cluster_id }}"

{# end if cloud gcp #}
{% endif %}
{% endif %}

{# end if host master #}
{% endif %}

event_watcher_config:
  # EventName:
  #   - pattern: 'regex pattern to match on the event's 'message' value'
  #     zbx_key: <zabbix key to report this 'event' on>
  #   - pattern: 'regex for a different event subtype'
  #     zbx_key: <zabbix key for event subtype>
  # OtherEvent:
  #   - pattern: 'regex for OtherEvent'
  #     zbx_key: <zabbix key to report>
  FailedScheduling:
    - pattern: failed to fit in any node
      zbx_key: openshift.master.cluster.event.failedscheduling
