---
- name: get oc version
  oc_version:
  register: oc_version

- name: set the kube version
  set_fact:
    kube_version: "{{ oc_version.results.kubernetes_short }}"

- name: get cert's basename
  set_fact:
    cert_basename: "{{ osalog_default_logging_cert | basename }}"
    key_basename: "{{ osalog_default_logging_key | basename }}"
    cacert_basename: "{{ osalog_default_logging_cacert | basename }}"

- name: copy logging certs to masters
  copy:
    src: "{{ item }}"
    dest: /etc/origin/master/named_certificates/
  with_items:
  - "{{ osalog_default_logging_cert }}"
  - "{{ osalog_default_logging_key }}"
  - "{{ osalog_default_logging_cacert }}"

- name: create project
  oadm_project:
    name: logging
    description: logging project
    display_name: logging
    description: Aggregated Logging
  register: projectout
  run_once: true
  retries: 12
  delay: 5

- debug: var=projectout

- name: label logging project to exclude from pv quotas
  oc_label:
    state: add
    kind: namespace
    name: logging
    labels: "{{ osalog_logging_project_labels }}"
  run_once: True
  when: not osalog_logging_labels

- name: Allow logging to deploy to all node types
  oc_edit:
    kind: ns
    name: logging
    separator: '#'
    content:
      metadata#annotations#openshift.io/node-selector: ""

- name: create secret
  oc_secret:
    namespace: logging
    name: logging-deployer
    files:
    - name: kibana.crt
      path: "/etc/origin/master/named_certificates/{{ cert_basename }}"
    - name: kibana.key
      path: "/etc/origin/master/named_certificates/{{ key_basename }}"
  register: secretout
  run_once: true
- debug: var=secretout

- name: Generate logging-deployer configmap
  template:
    src: logging-deployer.yaml.j2
    dest: /tmp/logging-deployer.yaml

- name: Create logging-deployer configmap
  command: oc create -f /tmp/logging-deployer.yaml -n logging

- name: create logging deployer template
  oc_obj:
    state: present
    namespace: logging
    name: logging-deployer-template
    kind: template
    files:
    - "/usr/share/ansible/openshift-ansible/roles/openshift_hosted_templates/files/v{{ kube_version }}/enterprise/logging-deployer.yaml"
  register: templateout
  run_once: true
- debug: var=templateout

- name: create template
  oc_process:
    namespace: logging
    template_name: logging-deployer-account-template
    create: True
    reconcile: False
  register: processout
  run_once: true
- debug: var=processout

- name: create role binding to logging-deployer service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:logging-deployer
    resource_kind: role
    resource_name: edit
  register: policyout
  run_once: true
- debug: var=policyout

- name: add cluster role binding to logging-deployer service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:logging-deployer
    resource_kind: cluster-role
    resource_name: oauth-editor
  register: policyout
  run_once: true
- debug: var=policyout

- name: add scc to aggregated-logging-fluentd service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:aggregated-logging-fluentd
    resource_kind: scc
    resource_name: privileged
  register: policyout
  run_once: true
- debug: var=policyout

- name: add cluster role binding to aggregated-logging-fluentd service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:aggregated-logging-fluentd
    resource_kind: cluster-role
    resource_name: cluster-reader
  register: policyout
  run_once: true
- debug: var=policyout

- name: create template
  oc_process:
    namespace: logging
    template_name: logging-deployer-template
    create: True
    reconcile: False
    params:
      IMAGE_PREFIX: "{{ osalog_image_prefix }}"
      IMAGE_VERSION: "{{ osalog_image_version }}"
  register: processout
  run_once: true
- debug: var=processout

# wait 3 minutes for deployer pod to complete
- name: wait until logging deployer pod has completed
  oc_obj:
    state: list
    namespace: logging
    kind: pods
    selector: logging-infra=deployer
  register: podout
  until: podout.results.results[0]['items'][0].status.phase in ['Succeeded', 'Failed']
  retries: 36
  delay: 5
  run_once: true

- fail:
    msg: logging deployer failed to complete setup
  when: podout.results.results[0]['items'][0].status.phase != 'Succeeded'
  run_once: true

- name: add cluster role binding to aggregated-logging-elasticsearch service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:aggregated-logging-elasticsearch
    resource_kind: cluster-role
    resource_name: rolebinding-reader
  register: policyout
  run_once: true
- debug: var=policyout

- name: get deployer pod status
  oc_obj:
    state: list
    namespace: logging
    kind: pods
  register: podout
  run_once: true

- name: fetch the elastic search deploymentconfigs with selector
  oc_obj:
    state: list
    namespace: logging
    kind: dc
    selector: logging-infra=elasticsearch
  register: dcout
  run_once: true
- debug: var=dcout

- name: attach storage volumes to instances
  oc_pvc:
    namespace: logging
    name: "{{ item['metadata']['name'] }}"
    access_modes:
    - ReadWriteOnce
    volume_capacity: "{{ osalog_pv_size }}G"
  with_items: "{{ dcout.results.results[0]['items'] }}"
  register: pvcout
  run_once: true
- debug: var=pvcout

- name: wait until logging pvcs are bound completed
  oc_obj:
    state: list
    namespace: logging
    kind: pvc
  register: pvcout
  until:
  - pvcout.results.results[0]['items'][0].status.phase == 'Bound'
  - pvcout.results.results[0]['items'][1].status.phase == 'Bound'
  retries: 24
  delay: 5
  run_once: true

- name: attach storage volumes to instances
  oc_volume:
    namespace: logging
    kind: dc
    name: "{{ item['metadata']['name'] }}"
    mount_type: pvc
    claim_name: "{{ item['metadata']['name'] }}"
    claim_size: "{{ osalog_pv_size }}G"
    vol_name: elasticsearch-storage
  with_items: " {{ dcout.results.results[0]['items'] }}"
  register: volumeout
  run_once: true
- debug: var=volumeout

- name: Wait for elasticsearch first deploy to finish
  pause:
    seconds: 120
  run_once: true

- name: Redeploy elasticsearch
  command: "oc deploy --latest dc/{{ item['metadata']['name'] }} -n logging"
  with_items: " {{ dcout.results.results[0]['items'] }}"

- name: Wait for elasticsearch deploy to finish
  pause:
    seconds: 120
  run_once: true

- name: Remove old logging-fluentd configmap
  command: oc delete configmap/logging-fluentd -n logging

- name: Generate logging-fluentd configmap
  template:
    src: logging-fluentd.yaml.j2
    dest: /tmp/logging-fluentd.yaml

- name: Create logging-fluentd configmap
  command: oc create -f /tmp/logging-fluentd.yaml -n logging

- name: scale fluentd
  oc_label:
    selector: "{{ item }}"
    state: add
    kind: node
    labels:
      - key: logging-infra-fluentd
        value: 'true'
  with_items: " {{ osalog_fluentd_nodes }}"

- name: scale kibana frontend
  oc_scale:
    name: logging-kibana
    kind: dc
    namespace: logging
    replicas: 2
  register: scaleout
  run_once: true
- debug: var=scaleout

- name: add kibana url to master-config.yml
  yedit:
    src: /etc/origin/master/master-config.yaml
    key: assetConfig.loggingPublicURL
    value: "https://logs.{{ osalog_clusterid }}.openshift.com"
  register: yeditout
  notify:
  - restart openshift master services
- debug: var=yeditout

- name: Remove old logging-curator configmap
  command: oc delete configmap/logging-curator -n logging

- name: Generate logging-curator configmap
  template:
    src: logging-curator.yaml.j2
    dest: /tmp/logging-curator.yaml

- name: Create logging-curator configmap
  command: oc create -f /tmp/logging-curator.yaml -n logging

- name: Add the resource constraints to the curator
  oc_edit:
    kind: dc
    name: logging-curator
    namespace: logging
    content:
      spec.template.spec.containers[0].resources.limits.memory: "2G"
      spec.template.spec.containers[0].resources.requests.memory: "1G"

- name: Wait for curator to finish
  pause:
    seconds: 120
  run_once: true

- name: Redeploy curator
  command: oc deploy --latest dc/logging-curator -n logging

- name: Add the resource constraints to the metrics RC
  oc_edit:
    kind: dc
    name: logging-kibana
    namespace: logging
    content:
      # kibana
      spec.template.spec.containers[0].resources.limits.memory: "768M"
      spec.template.spec.containers[0].resources.requests.memory: "96M"
      # kibana-proxy
      spec.template.spec.containers[1].resources.limits.memory: "128M"
      spec.template.spec.containers[1].resources.requests.memory: "32M"

- name: Wait for kibana to finish
  pause:
    seconds: 120
  run_once: true

- name: Redeploy kibana
  command: oc deploy dc/logging-kibana --latest -n logging

- name: get ca from kibana secret
  oc_secret:
    state: list
    namespace: logging
    name: logging-kibana
    decode: True
  register: kibana_ca_cert
  run_once: true

- name: Configure certificates for reencrypt route
  oc_route:
    name: logging-kibana
    namespace: logging
    cert_path: "/etc/origin/master/named_certificates/{{ cert_basename }}"
    key_path: "/etc/origin/master/named_certificates/{{ key_basename }}"
    cacert_path: "/etc/origin/master/named_certificates/{{ cacert_basename }}"
    dest_cacert_content:  "{{ kibana_ca_cert.results.decoded['ca'] }}"
    service_name: logging-kibana
    host: "logs.{{ osalog_clusterid }}.openshift.com"
    tls_termination: reencrypt
  register: routeout
  run_once: true

- debug: var=routeout
  run_once: True
